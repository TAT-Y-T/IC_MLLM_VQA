{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2b73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 14:38:28.750681: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-19 14:38:29.343752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-19 14:38:29.343836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-19 14:38:29.448520: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-19 14:38:29.660283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-19 14:38:31.210887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bea136",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('derek-thomas/ScienceQA', split='test') # choose the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cae35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def build_filtered_test_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                                 split='test',\n",
    "                                 keep_grades='1-6'):\n",
    "    \"\"\"\n",
    "    构建按年级过滤的测试数据集（不限制样本数量）。\n",
    "\n",
    "    参数:\n",
    "        dataset_name (str): 数据集名称，例如 'derek-thomas/ScienceQA'。\n",
    "        split (str): 数据分割，例如 'train', 'test', 'validation'。\n",
    "        keep_grades (str or None): 筛选的年级段：\"1-6\"、\"7-12\" 或 None 表示不过滤。\n",
    "\n",
    "    返回:\n",
    "        List[Dict]: 筛选后的样本列表。\n",
    "    \"\"\"\n",
    "\n",
    "    def is_grade_allowed(grade_str):\n",
    "        if keep_grades is None:\n",
    "            return True\n",
    "        try:\n",
    "            grade_num = int(grade_str.replace(\"grade\", \"\"))\n",
    "            if keep_grades == \"1-6\":\n",
    "                return 1 <= grade_num <= 6\n",
    "            elif keep_grades == \"7-12\":\n",
    "                return 7 <= grade_num <= 12\n",
    "        except:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    data = load_dataset(dataset_name, split=split)\n",
    "    test_dataset = []\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        try:\n",
    "            if sample.get('question') is None:\n",
    "                continue\n",
    "\n",
    "            if not is_grade_allowed(sample.get(\"grade\", \"\")):\n",
    "                continue\n",
    "\n",
    "            solution = sample.get(\"solution\", \"\")\n",
    "            lecture = sample.get(\"lecture\", \"\")\n",
    "            solution_lecture = f\"{solution}\\n\\n{lecture}\".strip()\n",
    "\n",
    "            test_dataset.append({\n",
    "                \"image\": sample.get(\"image\", None), \n",
    "                \"question\": sample[\"question\"],\n",
    "                \"choices\": sample[\"choices\"],\n",
    "                # \"hint\": sample[\"hint\"],\n",
    "                \"answer\": sample[\"answer\"],\n",
    "                # \"solution_lecture\": solution_lecture,\n",
    "                'grade':sample[\"grade\"],\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"跳过第 {i} 个样本，错误：{e}\")\n",
    "            continue\n",
    "\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_filtered_test_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                                    split='validation',\n",
    "                                    keep_grades='1-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38005d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.54s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 设置设备：如果有 GPU 就用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 加载模型到 GPU\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    device_map={\"\": device}, # 明确放到 cuda 或 cpu\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32 # GPU 使用半精度更快\n",
    ")\n",
    "\n",
    "# 加载处理器\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 构造模型输入\n",
    "def build_message(sample):\n",
    "    content = []\n",
    "    if sample['image'] is not None:\n",
    "        content.append({\"type\": \"image\", \"image\": sample['image']})\n",
    "    question_text = f\"Question: {sample['question']}\\nChoices:\\n\"\n",
    "    for idx, choice in enumerate(sample['choices']):\n",
    "        question_text += f\"{chr(65 + idx)}. {choice}\\n\"\n",
    "    question_text += \"Please select the correct answer.\"\n",
    "    content.append({\"type\": \"text\", \"text\": question_text})\n",
    "    return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "# 解析模型输出中的选项字母\n",
    "def parse_output(output):\n",
    "    output = output.strip()\n",
    "    match = re.search(r\"\\b([A-D])[\\.\\:\\)]?\", output)\n",
    "    if match:\n",
    "        return ord(match.group(1)) - 65  # A -> 0, B -> 1, etc.\n",
    "    return -1\n",
    "\n",
    "# 存放评估记录\n",
    "all_records = []\n",
    "\n",
    "# 模型预测与评估\n",
    "for sample in tqdm(test_dataset):\n",
    "    messages = build_message(sample)\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs = [sample[\"image\"]] if sample[\"image\"] else None\n",
    "\n",
    "    inputs = processor(text=[text], images=image_inputs, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    output = processor.batch_decode(generated_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_answer = parse_output(output)\n",
    "\n",
    "    all_records.append({\n",
    "        \"Question\": sample[\"question\"],\n",
    "        \"Choices\": \"\\n\".join(sample[\"choices\"]),\n",
    "        \"True Answer\": sample[\"answer\"],\n",
    "        \"Predicted Answer\": pred_answer,\n",
    "        \"Model Output\": output,\n",
    "        \"Correct\": pred_answer == sample[\"answer\"]\n",
    "    })\n",
    "\n",
    "# 汇总与导出\n",
    "results_df = pd.DataFrame(all_records)\n",
    "acc = accuracy_score(results_df[\"True Answer\"], results_df[\"Predicted Answer\"])\n",
    "f1 = f1_score(results_df[\"True Answer\"], results_df[\"Predicted Answer\"], average='macro')\n",
    "\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 保存完整结果\n",
    "results_df.to_csv(\"Qwen3B_1_6_just_QA.csv\", index=False)\n",
    "\n",
    "# 导出错误案例方便分析\n",
    "error_df = results_df[results_df[\"Correct\"] == False]\n",
    "error_df.to_csv(\"Qwen3B_1_6_errors.csv\", index=False)\n",
    "\n",
    "# 可视化预测正确与否的分布（可选）\n",
    "results_df[\"Correct\"].value_counts().plot(kind='bar')\n",
    "plt.xticks([0, 1], ['Wrong', 'Correct'], rotation=0)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Prediction Correctness Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b85564",
   "metadata": {},
   "source": [
    "# with Hitn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfeaff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def build_filtered_test_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                                 split='test',\n",
    "                                 keep_grades='1-6'):\n",
    "    \"\"\"\n",
    "    构建按年级过滤的测试数据集（不限制样本数量）。\n",
    "\n",
    "    参数:\n",
    "        dataset_name (str): 数据集名称，例如 'derek-thomas/ScienceQA'。\n",
    "        split (str): 数据分割，例如 'train', 'test', 'validation'。\n",
    "        keep_grades (str or None): 筛选的年级段：\"1-6\"、\"7-12\" 或 None 表示不过滤。\n",
    "\n",
    "    返回:\n",
    "        List[Dict]: 筛选后的样本列表。\n",
    "    \"\"\"\n",
    "\n",
    "    def is_grade_allowed(grade_str):\n",
    "        if keep_grades is None:\n",
    "            return True\n",
    "        try:\n",
    "            grade_num = int(grade_str.replace(\"grade\", \"\").strip())\n",
    "            if keep_grades == \"1-6\":\n",
    "                return 1 <= grade_num <= 6\n",
    "            elif keep_grades == \"7-12\":\n",
    "                return 7 <= grade_num <= 12\n",
    "        except Exception:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    print(f\"Loading dataset: {dataset_name}, split: {split}\")\n",
    "    data = load_dataset(dataset_name, split=split)\n",
    "    test_dataset = []\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        try:\n",
    "            if sample.get('question') is None:\n",
    "                continue\n",
    "            if not is_grade_allowed(sample.get(\"grade\", \"\")):\n",
    "                continue\n",
    "\n",
    "            test_dataset.append({\n",
    "                \"image\": sample.get(\"image\", None), \n",
    "                \"question\": sample[\"question\"],\n",
    "                \"choices\": sample[\"choices\"],\n",
    "                \"hint\": sample.get(\"hint\", \"\"),\n",
    "                \"answer\": sample[\"answer\"],\n",
    "                \"grade\": sample.get(\"grade\", \"\")\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"跳过第 {i} 个样本，错误：{e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"最终保留样本数量: {len(test_dataset)}\")\n",
    "    return test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9156c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: derek-thomas/ScienceQA, split: test\n",
      "最终保留样本数量: 2724\n"
     ]
    }
   ],
   "source": [
    "test_dataset = build_filtered_test_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                                           split='test',\n",
    "                                           keep_grades='1-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce45f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=312x237>,\n",
       " 'question': 'Which better describes the Daintree rain forest ecosystem?',\n",
       " 'choices': ['It has year-round rain. It also has soil that is poor in nutrients.',\n",
       "  'It has cold winters. It also has many different types of organisms.'],\n",
       " 'hint': 'Figure: Daintree rain forest.\\nThe Daintree rain forest is a tropical rain forest ecosystem in northeastern Australia.',\n",
       " 'answer': 0,\n",
       " 'grade': 'grade3'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983a0562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Metrics:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "✅ 答案评估结果已保存为 Qwen3B_1_6_just_QA1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 构造模型输入（自动添加 hint）\n",
    "def build_message(sample):\n",
    "    content = []\n",
    "    if sample['image'] is not None:\n",
    "        content.append({\"type\": \"image\", \"image\": sample['image']})\n",
    "    \n",
    "    question_text = \"\"\n",
    "    if \"hint\" in sample and sample[\"hint\"]:\n",
    "        question_text += f\"Hint: {sample['hint']}\\n\\n\"\n",
    "\n",
    "    question_text += f\"Question: {sample['question']}\\nChoices:\\n\"\n",
    "    for idx, choice in enumerate(sample['choices']):\n",
    "        question_text += f\"{chr(65 + idx)}. {choice}\\n\"\n",
    "    question_text += \"Please select the correct answer.\"\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": question_text})\n",
    "    return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "# 解析模型输出中的选项字母\n",
    "def parse_output(output):\n",
    "    output = output.strip()\n",
    "\n",
    "    # 尝试匹配 \"Answer: A.\", \"Correct answer: B:\", \"B.\"\n",
    "    match = re.search(r\"(?:Answer\\s*[:\\-]?\\s*|Correct answer\\s*[:\\-]?\\s*|Option\\s*[:\\-]?\\s*)?([A-D])[\\.\\:]\", output, re.IGNORECASE)\n",
    "    if match:\n",
    "        return ord(match.group(1).upper()) - 65\n",
    "\n",
    "    # fallback: 匹配第一个单独出现的 A-D\n",
    "    match = re.search(r\"\\b([A-D])\\b\", output)\n",
    "    if match:\n",
    "        return ord(match.group(1).upper()) - 65\n",
    "\n",
    "    return -1\n",
    "\n",
    "# 存放评估记录\n",
    "all_records = []\n",
    "test_dataset = test_dataset[:2]\n",
    "# 模型预测与评估\n",
    "for sample in tqdm(test_dataset):\n",
    "    messages = build_message(sample)\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs = [sample[\"image\"]] if sample[\"image\"] else None\n",
    "\n",
    "    inputs = processor(text=[text], images=image_inputs, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    output = processor.batch_decode(generated_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_answer = parse_output(output)\n",
    "\n",
    "    all_records.append({\n",
    "        \"Question\": sample[\"question\"],\n",
    "        \"Choices\": \"\\n\".join(sample[\"choices\"]),\n",
    "        \"True Answer\": sample[\"answer\"],\n",
    "        \"Predicted Answer\": pred_answer,\n",
    "        \"Model Output\": output,\n",
    "        \"Correct\": pred_answer == sample[\"answer\"]\n",
    "    })\n",
    "\n",
    "# 汇总与导出\n",
    "results_df = pd.DataFrame(all_records)\n",
    "acc = accuracy_score(results_df[\"True Answer\"], results_df[\"Predicted Answer\"])\n",
    "f1 = f1_score(results_df[\"True Answer\"], results_df[\"Predicted Answer\"], average='macro')\n",
    "\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "results_df.to_csv(\"Qwen3B_1_6_just_QA.csv\", index=False)\n",
    "print(\"\\n✅ 答案评估结果已保存为 Qwen3B_1_6_just_QA1.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
