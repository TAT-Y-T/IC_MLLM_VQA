{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a382b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cd4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ç­›é€‰åçš„æ ·æœ¬æ•°é‡: 4349\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "def build_filtered_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                           split='train',\n",
    "                           keep_grades='1-6'):\n",
    "    \"\"\"\n",
    "    æ„å»ºæŒ‰å¹´çº§å’Œå›¾åƒå­˜åœ¨æ€§è¿‡æ»¤çš„æ•°æ®é›†ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        dataset_name (str): æ•°æ®é›†åç§°ï¼Œä¾‹å¦‚ 'derek-thomas/ScienceQA'ã€‚\n",
    "        split (str): æ•°æ®åˆ†å‰²ï¼Œä¾‹å¦‚ 'train', 'test', 'validation'ã€‚\n",
    "        keep_grades (str or None): ç­›é€‰çš„å¹´çº§æ®µï¼š\"1-6\"ã€\"7-12\" æˆ– None è¡¨ç¤ºä¸è¿‡æ»¤ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        List[Dict]: ç­›é€‰åçš„æ ·æœ¬åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def is_grade_allowed(grade_str):\n",
    "        if keep_grades is None:\n",
    "            return True\n",
    "        try:\n",
    "            grade_num = int(grade_str.replace(\"grade\", \"\"))\n",
    "            if keep_grades == \"1-6\":\n",
    "                return 1 <= grade_num <= 6\n",
    "            elif keep_grades == \"7-12\":\n",
    "                return 7 <= grade_num <= 12\n",
    "        except:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    data = load_dataset(dataset_name, split=split)\n",
    "    dataset = []\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        try:\n",
    "            if sample.get('question') is None:\n",
    "                continue\n",
    "            \n",
    "            if sample.get(\"image\", None) is None:\n",
    "                continue\n",
    "\n",
    "            if not is_grade_allowed(sample.get(\"grade\", \"\")):\n",
    "                continue\n",
    "\n",
    "            solution = sample.get(\"solution\", \"\")\n",
    "            lecture = sample.get(\"lecture\", \"\")\n",
    "            solution_lecture = f\"{solution}\\n\\n{lecture}\".strip()\n",
    "            \n",
    "            image = sample[\"image\"].convert(\"RGB\")\n",
    "            \n",
    "\n",
    "            # image = np.array(image)\n",
    "            # image = torch.tensor(image).permute(2, 0, 1)  # shape: (C, H, W)\n",
    "            dataset.append({\n",
    "                \"image\": image, \n",
    "                \"question\": sample[\"question\"],\n",
    "                \"choices\": sample[\"choices\"],\n",
    "                \"hint\": sample[\"hint\"],\n",
    "                \"answer\": sample[\"answer\"],\n",
    "                \"solution_lecture\": solution_lecture,\n",
    "                'grade':sample[\"grade\"],\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è·³è¿‡ç¬¬ {i} ä¸ªæ ·æœ¬ï¼Œé”™è¯¯ï¼š{e}\")\n",
    "            continue\n",
    "    return dataset\n",
    "\n",
    "dataset_train = build_filtered_dataset(split='train', keep_grades='1-6')\n",
    "print(f\"\\nâœ… ç­›é€‰åçš„æ ·æœ¬æ•°é‡: {len(dataset_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebb75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ç­›é€‰åçš„æ ·æœ¬æ•°é‡: 1481\n"
     ]
    }
   ],
   "source": [
    "dataset_val = build_filtered_dataset(split='validation', keep_grades='1-6')\n",
    "print(f\"\\nâœ… ç­›é€‰åçš„æ ·æœ¬æ•°é‡: {len(dataset_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f98ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which animal's limbs are also adapted for swimming?\n",
      "Choices: ['nine-banded armadillo', 'bottlenose dolphin']\n",
      "Hint: Harbor seals live along the coasts of the Atlantic and Pacific Oceans. They spend a lot of time hunting fish at sea.\n",
      "The  has four flippers for limbs. Its limbs are adapted for swimming.\n",
      "Figure: harbor seal.\n",
      "Grade: grade3\n",
      "Answer: 1\n",
      "Explanation: Look at the picture of the harbor seal.\n",
      "The harbor seal uses its flippers to push itself through water. The flippers can also help it change direction while swimming.\n",
      "Now look at each animal. Figure out which animal has a similar adaptation.\n",
      "The bottlenose dolphin has flippers. Its limbs are adapted for swimming.\n",
      "The nine-banded armadillo has short, thin legs. Its limbs are not adapted for swimming.\n",
      "\n",
      "An adaptation is an inherited trait that helps an organism survive or reproduce. Adaptations can include both body parts and behaviors.\n",
      "Arms, legs, flippers, and wings are different types of limbs. The type of limbs an animal has is an example of an adaptation. Animals' limbs can be adapted in different ways. For example, long legs might help an animal run fast. Flippers might help an animal swim. Wings might help an animal fly.\n",
      "Image type: <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "sample_1 = choice(dataset_train)\n",
    "print(f\"Question: {sample_1['question']}\")\n",
    "print(f\"Choices: {sample_1['choices']}\")\n",
    "print(f\"Hint: {sample_1['hint']}\")\n",
    "print(f\"Grade: {sample_1['grade']}\")\n",
    "print(f\"Answer: {sample_1['answer']}\")\n",
    "print(f\"Explanation: {sample_1['solution_lecture']}\")\n",
    "print(f\"Image type: {type(sample_1['image'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a5efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class QwenVLPrefixDataset(Dataset):\n",
    "    def __init__(self, data_list, processor, debug=False):\n",
    "        self.data_list = data_list\n",
    "        self.processor = processor\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_list[idx]\n",
    "        return self.build_training_sample(sample)\n",
    "\n",
    "    def build_training_sample(self, sample):\n",
    "        # 1. æ„é€ ç­”æ¡ˆ\n",
    "        if isinstance(sample[\"answer\"], int):\n",
    "            answer_index = sample[\"answer\"]\n",
    "        else:\n",
    "            answer_index = sample[\"choices\"].index(sample[\"answer\"])\n",
    "        answer_letter = chr(65 + answer_index)\n",
    "\n",
    "        # 2. æ„å»ºé—®é¢˜å†…å®¹\n",
    "        question_text = f\"Question: {sample['question']}\\nChoices:\\n\"\n",
    "        for idx, choice in enumerate(sample[\"choices\"]):\n",
    "            question_text += f\"{chr(65 + idx)}. {choice}\\n\"\n",
    "        if sample.get(\"hint\"):\n",
    "            question_text += f\"\\nHint: {sample['hint']}\\n\"\n",
    "        question_text += (\n",
    "            \"\\nHere is a image:\\n\"\n",
    "            \"Please select the correct answer. Then, explain your reasoning in detail. \"\n",
    "            \"Make sure your explanation is at least three sentences long, \"\n",
    "            \"refers to specific data from the image, and shows your step-by-step logic.\"\n",
    "        )\n",
    "\n",
    "        # 3. æ„å»º label æ–‡æœ¬ï¼ˆæ‹¼æ¥åˆ° prompt åï¼‰\n",
    "        label_text = f\"\\nAnswer: {answer_letter}\\nExplanation: {sample['solution_lecture']}\"\n",
    "\n",
    "        # 4. å›¾åƒå¤„ç†\n",
    "        image = sample[\"image\"]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise ValueError(\"image must be a PIL.Image.Image\")\n",
    "        image = image.convert(\"RGB\").resize((224, 224))\n",
    "\n",
    "        # 5. æ„é€ å®Œæ•´å¯¹è¯ + å›¾åƒè¾“å…¥\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question_text},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": label_text}\n",
    "        ]\n",
    "\n",
    "        # 6. ç¼–ç å›¾æ–‡è¾“å…¥ï¼ˆæ³¨æ„ï¼šä¸€ä½“ç¼–ç  prompt+labelï¼‰\n",
    "        text = self.processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "        inputs = self.processor(\n",
    "            text=text,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=False\n",
    "        )\n",
    "\n",
    "        # 7. æ„é€  labels â€”â€” ç”¨ input_ids å…‹éš†ï¼Œå¹¶ mask æ‰å‰é¢çš„ prompt\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # æ‰¾åˆ° label çš„èµ·å§‹ä½ç½®ï¼šç”¨ processor å†åª encode user éƒ¨åˆ†è®¡ç®—é•¿åº¦\n",
    "        user_text_only = self.processor.apply_chat_template(chat[:1], tokenize=False, add_generation_prompt=False)\n",
    "        user_input_ids = self.processor.tokenizer(user_text_only, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "        user_len = user_input_ids.shape[-1]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:user_len] = -100  # mask prompt éƒ¨åˆ†\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "        if \"image_grid_thw\" in inputs:\n",
    "            result[\"image_grid_thw\"] = inputs[\"image_grid_thw\"].squeeze(0)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] input_ids.shape: {input_ids.shape}\")\n",
    "            print(f\"[DEBUG] labels.shape: {labels.shape}\")\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5519615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "train_dataset = QwenVLPrefixDataset(dataset_train, processor, debug=False)\n",
    "dataloader = DataLoader(dataset_train, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8434da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def qwen_vl_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate å‡½æ•°ï¼Œç”¨äº QwenVL å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒã€‚\n",
    "    è‡ªåŠ¨å¯¹ input_ids / labels è¿›è¡Œ paddingï¼Œ\n",
    "    pixel_values å’Œ image_grid_thw ç›´æ¥ stackï¼ˆå‡è®¾ shape ä¸€è‡´ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    def pad_tensor_list(tensor_list, pad_value=0):\n",
    "        return pad_sequence(tensor_list, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # éƒ¨åˆ†æ¨¡å‹ forward éœ€è¦ attention_maskï¼Œå°½ç®¡æœ‰äº›æ¨¡å‹ä¸å¼ºä¾èµ–ï¼Œä¹Ÿå»ºè®®ä¿ç•™\n",
    "    input_ids = pad_tensor_list(input_ids, pad_value=0)\n",
    "    attention_mask = pad_tensor_list(attention_mask, pad_value=0)\n",
    "    labels = pad_tensor_list(labels, pad_value=-100)  # é¿å… loss å¯¹ pad token è®¡ç®—\n",
    "\n",
    "    # å¤šæ¨¡æ€éƒ¨åˆ†\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    \n",
    "    # å¯é€‰ï¼šå®¹é”™å¤„ç† image_grid_thwï¼ˆæœ‰äº›æ¨¡å‹å¯èƒ½æ²¡æœ‰ï¼‰\n",
    "    if \"image_grid_thw\" in batch[0]:\n",
    "        image_grid_thw = torch.stack([item[\"image_grid_thw\"] for item in batch])\n",
    "    else:\n",
    "        image_grid_thw = None\n",
    "\n",
    "    # æ„é€ è¿”å›å­—å…¸\n",
    "    result = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    # print(f\"input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}, labels shape: {labels.shape}, pixel_values shape: {pixel_values.shape}\")\n",
    "    if image_grid_thw is not None:\n",
    "        result[\"image_grid_thw\"] = image_grid_thw\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79562971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ç¬¬ä¸€ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ ====\n",
      "input_ids shape: torch.Size([270])\n",
      "attention_mask shape: torch.Size([270])\n",
      "labels shape: torch.Size([270])\n",
      "pixel_values shape: torch.Size([256, 1176])\n",
      "image_grid_thw shape: torch.Size([3])\n",
      "\n",
      "--- è§£ç åçš„ input_ids ---\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Question: Which of these states is farthest north?\n",
      "Choices:\n",
      "A. West Virginia\n",
      "B. Louisiana\n",
      "C. Arizona\n",
      "D. Oklahoma\n",
      "\n",
      "Here is a image:\n",
      "Please select the correct answer. Then, explain your reasoning in detail. Make sure your explanation is at least three sentences long, refers to specific data from the image, and shows your step-by-step logic.\n",
      "assistant\n",
      "\n",
      "Answer: A\n",
      "Explanation: To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.\n",
      "\n",
      "Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\n",
      "A compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\n",
      "The north arrow points to the North Pole. On most maps, north is at the top of the map.\n",
      "\n",
      "\n",
      "--- è§£ç åçš„ labels ---\n",
      "\n",
      "assistant\n",
      "\n",
      "Answer: A\n",
      "Explanation: To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.\n",
      "\n",
      "Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\n",
      "A compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\n",
      "The north arrow points to the North Pole. On most maps, north is at the top of the map.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# æ­£ç¡®ä½¿ç”¨ collate_fn\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=qwen_vl_collate_fn)\n",
    "\n",
    "# è·å–ç¬¬ä¸€ä¸ª batch\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æ„\n",
    "n = 0\n",
    "print(\"==== ç¬¬ä¸€ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ ====\")\n",
    "print(f\"input_ids shape: {first_batch['input_ids'][n].shape}\")\n",
    "print(f\"attention_mask shape: {first_batch['attention_mask'][0].shape}\")\n",
    "print(f\"labels shape: {first_batch['labels'][n].shape}\")\n",
    "print(f\"pixel_values shape: {first_batch['pixel_values'][n].shape}\")\n",
    "print(f\"image_grid_thw shape: {first_batch['image_grid_thw'][n].shape}\")\n",
    "\n",
    "# å¯é€‰ï¼šæŸ¥çœ‹æ–‡æœ¬å†…å®¹ï¼ˆéœ€è¦ tokenizerï¼‰\n",
    "tokenizer = processor.tokenizer\n",
    "decoded_input = tokenizer.decode(first_batch['input_ids'][n], skip_special_tokens=True)\n",
    "decoded_label = tokenizer.decode(\n",
    "    [id for id in first_batch['labels'][n].tolist() if id != -100],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- è§£ç åçš„ input_ids ---\")\n",
    "print(decoded_input)\n",
    "\n",
    "print(\"\\n--- è§£ç åçš„ labels ---\")\n",
    "print(decoded_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fbbc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db5f01e10ca40d1a3d8d99ebec86836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c7ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,294,689,792 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # å¯¹äº Qwen è¿™ç±»ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨ CAUSAL_LM\n",
    "    inference_mode=False,          # è¡¨ç¤ºç”¨äºè®­ç»ƒï¼Œä¸æ˜¯æ¨ç†\n",
    "    r=8,                           # LoRA ç§©ï¼Œè¶Šå¤§å¯è®­ç»ƒå‚æ•°è¶Šå¤šï¼ˆå¸¸è§ï¼š4, 8, 16ï¼‰\n",
    "    lora_alpha=32,                 # ç¼©æ”¾å› å­ï¼Œé€šå¸¸ä¸ r åŒé˜¶æˆ–æ›´å¤§\n",
    "    lora_dropout=0.1,              # dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796f8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                step = self.state.global_step\n",
    "                print(f\"\\nâŒ [OOM at step {step}] Skipping this batch.\")\n",
    "                input_ids = inputs.get(\"input_ids\")\n",
    "                labels = inputs.get(\"labels\")\n",
    "\n",
    "                if input_ids is not None:\n",
    "                    print(f\"    input_ids shape: {input_ids.shape}\")\n",
    "                if labels is not None:\n",
    "                    print(f\"    labels shape: {labels.shape}\")\n",
    "\n",
    "                # æ¸…ç†æ˜¾å­˜\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # === âœ… ä¿å­˜å½“å‰ checkpoint ===\n",
    "                oom_ckpt_dir = os.path.join(self.args.output_dir, f\"checkpoint-oom-step-{step}\")\n",
    "                print(f\"ğŸ’¾ Saving checkpoint due to OOM at: {oom_ckpt_dir}\")\n",
    "                self.save_model(oom_ckpt_dir)\n",
    "\n",
    "                dummy_loss = torch.tensor(0.0, requires_grad=True).to(model.device)\n",
    "                return (dummy_loss, None) if return_outputs else dummy_loss\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14060ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "def parse_output(output: str):\n",
    "    \"\"\"\n",
    "    æå–é€‰æ‹©é¢˜ç­”æ¡ˆå’Œè§£é‡Šæ–‡æœ¬ï¼Œå®¹é”™æ”¯æŒ Answer: A, A., A: ç­‰ã€‚\n",
    "    æ”¯æŒé€‰é¡¹ A-Fã€‚\n",
    "    è¿”å› answer: int (0~5 å¯¹åº” A-F), explanation: str\n",
    "    \"\"\"\n",
    "    output = output.strip()\n",
    "\n",
    "    # ä¼˜å…ˆåŒ¹é… Answer: A ç­‰æ ¼å¼\n",
    "    answer_match = re.search(r\"(?i)\\banswer\\s*[:\\-]?\\s*([A-F])\\b\", output)\n",
    "    if not answer_match:\n",
    "        # å›é€€åŒ¹é… A. / A: / A- ç­‰æ ¼å¼\n",
    "        answer_match = re.search(r\"\\b([A-F])[\\.\\:\\-]\", output)\n",
    "\n",
    "    if answer_match:\n",
    "        choice_char = answer_match.group(1).upper()\n",
    "        answer = ord(choice_char) - ord(\"A\")\n",
    "    else:\n",
    "        answer = -1\n",
    "\n",
    "    explanation = \"\"\n",
    "    if answer_match:\n",
    "        idx = output.find(answer_match.group(0))\n",
    "        if idx != -1:\n",
    "            explanation = output[idx + len(answer_match.group(0)):].strip()\n",
    "\n",
    "    return answer, explanation\n",
    "\n",
    "\n",
    "def keyword_overlap(pred, ref):\n",
    "    pred_keywords = set(pred.lower().split())\n",
    "    ref_keywords = set(ref.lower().split())\n",
    "    if not ref_keywords:\n",
    "        return 0.0\n",
    "    return len(pred_keywords & ref_keywords) / len(ref_keywords)\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    predictions = eval_preds.predictions\n",
    "    label_ids = eval_preds.label_ids\n",
    "\n",
    "    # å¦‚æœ predictions æ˜¯ tupleï¼ˆä¾‹å¦‚åŒ…å« logitsï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªä½œä¸º token ids\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # å¦‚æœæ˜¯ logitsï¼ˆå½¢å¦‚ [batch, seq_len, vocab_size]ï¼‰ï¼Œåˆ™ argmax å¾—åˆ° token ids\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = predictions.argmax(-1)\n",
    "\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "\n",
    "    for pred_ids, label in zip(predictions, label_ids):\n",
    "        label = [id for id in label if id != -100]\n",
    "        decoded_pred = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "        decoded_label = tokenizer.decode(label, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds.append(decoded_pred.strip())\n",
    "        decoded_labels.append(decoded_label.strip())\n",
    "\n",
    "    # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹å’Œæ ‡ç­¾\n",
    "    # print(\"\\n==== ç¤ºä¾‹è¾“å‡º ====\")\n",
    "    # print(\"é¢„æµ‹ï¼š\", decoded_preds[0])\n",
    "    # print(\"æ ‡ç­¾ï¼š\", decoded_labels[0])\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1_scores = []\n",
    "    bleu4_scores = []\n",
    "    rouge_l_scores = []\n",
    "    keyword_overlaps = []\n",
    "    choice_correct = []\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        reference = label.split()\n",
    "        candidate = pred.split()\n",
    "\n",
    "        # BLEU-1 å’Œ BLEU-4\n",
    "        bleu1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu4 = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "        # ROUGE-L\n",
    "        try:\n",
    "            rouge_l = rouge.get_scores(pred, label)[0]['rouge-l']['f']\n",
    "        except ValueError:\n",
    "            rouge_l = 0.0\n",
    "        rouge_l_scores.append(rouge_l)\n",
    "\n",
    "        # Keyword overlap\n",
    "        keyword_acc = keyword_overlap(pred, label)\n",
    "        keyword_overlaps.append(keyword_acc)\n",
    "\n",
    "        # Choice accuracy\n",
    "        pred_choice, _ = parse_output(pred)\n",
    "        label_choice, _ = parse_output(label)\n",
    "        is_correct = (pred_choice == label_choice) and (pred_choice != -1)\n",
    "        choice_correct.append(int(is_correct))\n",
    "\n",
    "    return {\n",
    "        \"BLEU-1\": np.mean(bleu1_scores),\n",
    "        \"BLEU-4\": np.mean(bleu4_scores),\n",
    "        \"ROUGE-L\": np.mean(rouge_l_scores),\n",
    "        \"KeywordOverlap\": np.mean(keyword_overlaps),\n",
    "        \"ChoiceAccuracy\": np.mean(choice_correct),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3586bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "small_train_dataset=dataset_train[:100]\n",
    "small_val_dataset=dataset_val[:5]\n",
    "train_dataset = QwenVLPrefixDataset(small_train_dataset, processor, debug=False)\n",
    "val_dataset = QwenVLPrefixDataset(small_val_dataset, processor, debug=False)    \n",
    "# dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804580d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_354847/448615443.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[codecarbon ERROR @ 14:48:35] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5vl-Lora1-6_0610\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20, \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1, \n",
    "    dataloader_num_workers=0,\n",
    "    eval_accumulation_steps=1,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,  # å¦‚æœä½¿ç”¨çš„æ˜¯æ”¯æŒ bfloat16 çš„ GPUï¼Œå¯æ”¹ä¸º bf16=True\n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=qwen_vl_collate_fn,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer=processor.tokenizer),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21c2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:48:35] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:49, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu-1</th>\n",
       "      <th>Bleu-4</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Keywordoverlap</th>\n",
       "      <th>Choiceaccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.882800</td>\n",
       "      <td>2.933081</td>\n",
       "      <td>0.633390</td>\n",
       "      <td>0.551097</td>\n",
       "      <td>0.761899</td>\n",
       "      <td>0.921052</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.640300</td>\n",
       "      <td>2.871847</td>\n",
       "      <td>0.590472</td>\n",
       "      <td>0.518662</td>\n",
       "      <td>0.741928</td>\n",
       "      <td>0.933462</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.638300</td>\n",
       "      <td>2.878922</td>\n",
       "      <td>0.585641</td>\n",
       "      <td>0.522042</td>\n",
       "      <td>0.733815</td>\n",
       "      <td>0.937381</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:59:34] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=3.1733709653218587, metrics={'train_runtime': 658.9777, 'train_samples_per_second': 0.759, 'train_steps_per_second': 0.091, 'total_flos': 8590162057900032.0, 'train_loss': 3.1733709653218587, 'epoch': 4.64})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cc9f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./qwen2.5vl-Lora1-6/final_0609_10epoch\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
