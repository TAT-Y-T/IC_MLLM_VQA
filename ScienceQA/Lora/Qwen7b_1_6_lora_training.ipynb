{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a382b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cd4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 筛选后的样本数量: 4349\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "def build_filtered_dataset(dataset_name='derek-thomas/ScienceQA',\n",
    "                           split='train',\n",
    "                           keep_grades='1-6'):\n",
    "    \"\"\"\n",
    "    构建按年级和图像存在性过滤的数据集。\n",
    "\n",
    "    参数:\n",
    "        dataset_name (str): 数据集名称，例如 'derek-thomas/ScienceQA'。\n",
    "        split (str): 数据分割，例如 'train', 'test', 'validation'。\n",
    "        keep_grades (str or None): 筛选的年级段：\"1-6\"、\"7-12\" 或 None 表示不过滤。\n",
    "\n",
    "    返回:\n",
    "        List[Dict]: 筛选后的样本列表。\n",
    "    \"\"\"\n",
    "\n",
    "    def is_grade_allowed(grade_str):\n",
    "        if keep_grades is None:\n",
    "            return True\n",
    "        try:\n",
    "            grade_num = int(grade_str.replace(\"grade\", \"\"))\n",
    "            if keep_grades == \"1-6\":\n",
    "                return 1 <= grade_num <= 6\n",
    "            elif keep_grades == \"7-12\":\n",
    "                return 7 <= grade_num <= 12\n",
    "        except:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    data = load_dataset(dataset_name, split=split)\n",
    "    dataset = []\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        try:\n",
    "            if sample.get('question') is None:\n",
    "                continue\n",
    "            \n",
    "            if sample.get(\"image\", None) is None:\n",
    "                continue\n",
    "\n",
    "            if not is_grade_allowed(sample.get(\"grade\", \"\")):\n",
    "                continue\n",
    "\n",
    "            solution = sample.get(\"solution\", \"\")\n",
    "            lecture = sample.get(\"lecture\", \"\")\n",
    "            solution_lecture = f\"{solution}\\n\\n{lecture}\".strip()\n",
    "            \n",
    "            image = sample[\"image\"].convert(\"RGB\")\n",
    "            \n",
    "\n",
    "            # image = np.array(image)\n",
    "            # image = torch.tensor(image).permute(2, 0, 1)  # shape: (C, H, W)\n",
    "            dataset.append({\n",
    "                \"image\": image, \n",
    "                \"question\": sample[\"question\"],\n",
    "                \"choices\": sample[\"choices\"],\n",
    "                \"hint\": sample[\"hint\"],\n",
    "                \"answer\": sample[\"answer\"],\n",
    "                \"solution_lecture\": solution_lecture,\n",
    "                'grade':sample[\"grade\"],\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"跳过第 {i} 个样本，错误：{e}\")\n",
    "            continue\n",
    "    return dataset\n",
    "\n",
    "dataset_train = build_filtered_dataset(split='train', keep_grades='1-6')\n",
    "print(f\"\\n✅ 筛选后的样本数量: {len(dataset_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebb75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 筛选后的样本数量: 1481\n"
     ]
    }
   ],
   "source": [
    "dataset_val = build_filtered_dataset(split='validation', keep_grades='1-6')\n",
    "print(f\"\\n✅ 筛选后的样本数量: {len(dataset_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f98ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which animal's limbs are also adapted for swimming?\n",
      "Choices: ['nine-banded armadillo', 'bottlenose dolphin']\n",
      "Hint: Harbor seals live along the coasts of the Atlantic and Pacific Oceans. They spend a lot of time hunting fish at sea.\n",
      "The  has four flippers for limbs. Its limbs are adapted for swimming.\n",
      "Figure: harbor seal.\n",
      "Grade: grade3\n",
      "Answer: 1\n",
      "Explanation: Look at the picture of the harbor seal.\n",
      "The harbor seal uses its flippers to push itself through water. The flippers can also help it change direction while swimming.\n",
      "Now look at each animal. Figure out which animal has a similar adaptation.\n",
      "The bottlenose dolphin has flippers. Its limbs are adapted for swimming.\n",
      "The nine-banded armadillo has short, thin legs. Its limbs are not adapted for swimming.\n",
      "\n",
      "An adaptation is an inherited trait that helps an organism survive or reproduce. Adaptations can include both body parts and behaviors.\n",
      "Arms, legs, flippers, and wings are different types of limbs. The type of limbs an animal has is an example of an adaptation. Animals' limbs can be adapted in different ways. For example, long legs might help an animal run fast. Flippers might help an animal swim. Wings might help an animal fly.\n",
      "Image type: <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "sample_1 = choice(dataset_train)\n",
    "print(f\"Question: {sample_1['question']}\")\n",
    "print(f\"Choices: {sample_1['choices']}\")\n",
    "print(f\"Hint: {sample_1['hint']}\")\n",
    "print(f\"Grade: {sample_1['grade']}\")\n",
    "print(f\"Answer: {sample_1['answer']}\")\n",
    "print(f\"Explanation: {sample_1['solution_lecture']}\")\n",
    "print(f\"Image type: {type(sample_1['image'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a5efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class QwenVLPrefixDataset(Dataset):\n",
    "    def __init__(self, data_list, processor, debug=False):\n",
    "        self.data_list = data_list\n",
    "        self.processor = processor\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_list[idx]\n",
    "        return self.build_training_sample(sample)\n",
    "\n",
    "    def build_training_sample(self, sample):\n",
    "        # 1. 构造答案\n",
    "        if isinstance(sample[\"answer\"], int):\n",
    "            answer_index = sample[\"answer\"]\n",
    "        else:\n",
    "            answer_index = sample[\"choices\"].index(sample[\"answer\"])\n",
    "        answer_letter = chr(65 + answer_index)\n",
    "\n",
    "        # 2. 构建问题内容\n",
    "        question_text = f\"Question: {sample['question']}\\nChoices:\\n\"\n",
    "        for idx, choice in enumerate(sample[\"choices\"]):\n",
    "            question_text += f\"{chr(65 + idx)}. {choice}\\n\"\n",
    "        if sample.get(\"hint\"):\n",
    "            question_text += f\"\\nHint: {sample['hint']}\\n\"\n",
    "        question_text += (\n",
    "            \"\\nHere is a image:\\n\"\n",
    "            \"Please select the correct answer. Then, explain your reasoning in detail. \"\n",
    "            \"Make sure your explanation is at least three sentences long, \"\n",
    "            \"refers to specific data from the image, and shows your step-by-step logic.\"\n",
    "        )\n",
    "\n",
    "        # 3. 构建 label 文本（拼接到 prompt 后）\n",
    "        label_text = f\"\\nAnswer: {answer_letter}\\nExplanation: {sample['solution_lecture']}\"\n",
    "\n",
    "        # 4. 图像处理\n",
    "        image = sample[\"image\"]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise ValueError(\"image must be a PIL.Image.Image\")\n",
    "        image = image.convert(\"RGB\").resize((224, 224))\n",
    "\n",
    "        # 5. 构造完整对话 + 图像输入\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question_text},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": label_text}\n",
    "        ]\n",
    "\n",
    "        # 6. 编码图文输入（注意：一体编码 prompt+label）\n",
    "        text = self.processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "        inputs = self.processor(\n",
    "            text=text,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=False\n",
    "        )\n",
    "\n",
    "        # 7. 构造 labels —— 用 input_ids 克隆，并 mask 掉前面的 prompt\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # 找到 label 的起始位置：用 processor 再只 encode user 部分计算长度\n",
    "        user_text_only = self.processor.apply_chat_template(chat[:1], tokenize=False, add_generation_prompt=False)\n",
    "        user_input_ids = self.processor.tokenizer(user_text_only, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "        user_len = user_input_ids.shape[-1]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:user_len] = -100  # mask prompt 部分\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "        if \"image_grid_thw\" in inputs:\n",
    "            result[\"image_grid_thw\"] = inputs[\"image_grid_thw\"].squeeze(0)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] input_ids.shape: {input_ids.shape}\")\n",
    "            print(f\"[DEBUG] labels.shape: {labels.shape}\")\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5519615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "train_dataset = QwenVLPrefixDataset(dataset_train, processor, debug=False)\n",
    "dataloader = DataLoader(dataset_train, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8434da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def qwen_vl_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate 函数，用于 QwenVL 多模态模型训练。\n",
    "    自动对 input_ids / labels 进行 padding，\n",
    "    pixel_values 和 image_grid_thw 直接 stack（假设 shape 一致）。\n",
    "    \"\"\"\n",
    "    def pad_tensor_list(tensor_list, pad_value=0):\n",
    "        return pad_sequence(tensor_list, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # 部分模型 forward 需要 attention_mask，尽管有些模型不强依赖，也建议保留\n",
    "    input_ids = pad_tensor_list(input_ids, pad_value=0)\n",
    "    attention_mask = pad_tensor_list(attention_mask, pad_value=0)\n",
    "    labels = pad_tensor_list(labels, pad_value=-100)  # 避免 loss 对 pad token 计算\n",
    "\n",
    "    # 多模态部分\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    \n",
    "    # 可选：容错处理 image_grid_thw（有些模型可能没有）\n",
    "    if \"image_grid_thw\" in batch[0]:\n",
    "        image_grid_thw = torch.stack([item[\"image_grid_thw\"] for item in batch])\n",
    "    else:\n",
    "        image_grid_thw = None\n",
    "\n",
    "    # 构造返回字典\n",
    "    result = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    # print(f\"input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}, labels shape: {labels.shape}, pixel_values shape: {pixel_values.shape}\")\n",
    "    if image_grid_thw is not None:\n",
    "        result[\"image_grid_thw\"] = image_grid_thw\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79562971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 第一个 batch 的第一个样本 ====\n",
      "input_ids shape: torch.Size([270])\n",
      "attention_mask shape: torch.Size([270])\n",
      "labels shape: torch.Size([270])\n",
      "pixel_values shape: torch.Size([256, 1176])\n",
      "image_grid_thw shape: torch.Size([3])\n",
      "\n",
      "--- 解码后的 input_ids ---\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Question: Which of these states is farthest north?\n",
      "Choices:\n",
      "A. West Virginia\n",
      "B. Louisiana\n",
      "C. Arizona\n",
      "D. Oklahoma\n",
      "\n",
      "Here is a image:\n",
      "Please select the correct answer. Then, explain your reasoning in detail. Make sure your explanation is at least three sentences long, refers to specific data from the image, and shows your step-by-step logic.\n",
      "assistant\n",
      "\n",
      "Answer: A\n",
      "Explanation: To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.\n",
      "\n",
      "Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\n",
      "A compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\n",
      "The north arrow points to the North Pole. On most maps, north is at the top of the map.\n",
      "\n",
      "\n",
      "--- 解码后的 labels ---\n",
      "\n",
      "assistant\n",
      "\n",
      "Answer: A\n",
      "Explanation: To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.\n",
      "\n",
      "Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\n",
      "A compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\n",
      "The north arrow points to the North Pole. On most maps, north is at the top of the map.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 正确使用 collate_fn\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=qwen_vl_collate_fn)\n",
    "\n",
    "# 获取第一个 batch\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# 查看第一个样本的结构\n",
    "n = 0\n",
    "print(\"==== 第一个 batch 的第一个样本 ====\")\n",
    "print(f\"input_ids shape: {first_batch['input_ids'][n].shape}\")\n",
    "print(f\"attention_mask shape: {first_batch['attention_mask'][0].shape}\")\n",
    "print(f\"labels shape: {first_batch['labels'][n].shape}\")\n",
    "print(f\"pixel_values shape: {first_batch['pixel_values'][n].shape}\")\n",
    "print(f\"image_grid_thw shape: {first_batch['image_grid_thw'][n].shape}\")\n",
    "\n",
    "# 可选：查看文本内容（需要 tokenizer）\n",
    "tokenizer = processor.tokenizer\n",
    "decoded_input = tokenizer.decode(first_batch['input_ids'][n], skip_special_tokens=True)\n",
    "decoded_label = tokenizer.decode(\n",
    "    [id for id in first_batch['labels'][n].tolist() if id != -100],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- 解码后的 input_ids ---\")\n",
    "print(decoded_input)\n",
    "\n",
    "print(\"\\n--- 解码后的 labels ---\")\n",
    "print(decoded_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fbbc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db5f01e10ca40d1a3d8d99ebec86836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c7ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,294,689,792 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # 对于 Qwen 这类生成模型，使用 CAUSAL_LM\n",
    "    inference_mode=False,          # 表示用于训练，不是推理\n",
    "    r=8,                           # LoRA 秩，越大可训练参数越多（常见：4, 8, 16）\n",
    "    lora_alpha=32,                 # 缩放因子，通常与 r 同阶或更大\n",
    "    lora_dropout=0.1,              # dropout，防止过拟合\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796f8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                step = self.state.global_step\n",
    "                print(f\"\\n❌ [OOM at step {step}] Skipping this batch.\")\n",
    "                input_ids = inputs.get(\"input_ids\")\n",
    "                labels = inputs.get(\"labels\")\n",
    "\n",
    "                if input_ids is not None:\n",
    "                    print(f\"    input_ids shape: {input_ids.shape}\")\n",
    "                if labels is not None:\n",
    "                    print(f\"    labels shape: {labels.shape}\")\n",
    "\n",
    "                # 清理显存\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # === ✅ 保存当前 checkpoint ===\n",
    "                oom_ckpt_dir = os.path.join(self.args.output_dir, f\"checkpoint-oom-step-{step}\")\n",
    "                print(f\"💾 Saving checkpoint due to OOM at: {oom_ckpt_dir}\")\n",
    "                self.save_model(oom_ckpt_dir)\n",
    "\n",
    "                dummy_loss = torch.tensor(0.0, requires_grad=True).to(model.device)\n",
    "                return (dummy_loss, None) if return_outputs else dummy_loss\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14060ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "def parse_output(output: str):\n",
    "    \"\"\"\n",
    "    提取选择题答案和解释文本，容错支持 Answer: A, A., A: 等。\n",
    "    支持选项 A-F。\n",
    "    返回 answer: int (0~5 对应 A-F), explanation: str\n",
    "    \"\"\"\n",
    "    output = output.strip()\n",
    "\n",
    "    # 优先匹配 Answer: A 等格式\n",
    "    answer_match = re.search(r\"(?i)\\banswer\\s*[:\\-]?\\s*([A-F])\\b\", output)\n",
    "    if not answer_match:\n",
    "        # 回退匹配 A. / A: / A- 等格式\n",
    "        answer_match = re.search(r\"\\b([A-F])[\\.\\:\\-]\", output)\n",
    "\n",
    "    if answer_match:\n",
    "        choice_char = answer_match.group(1).upper()\n",
    "        answer = ord(choice_char) - ord(\"A\")\n",
    "    else:\n",
    "        answer = -1\n",
    "\n",
    "    explanation = \"\"\n",
    "    if answer_match:\n",
    "        idx = output.find(answer_match.group(0))\n",
    "        if idx != -1:\n",
    "            explanation = output[idx + len(answer_match.group(0)):].strip()\n",
    "\n",
    "    return answer, explanation\n",
    "\n",
    "\n",
    "def keyword_overlap(pred, ref):\n",
    "    pred_keywords = set(pred.lower().split())\n",
    "    ref_keywords = set(ref.lower().split())\n",
    "    if not ref_keywords:\n",
    "        return 0.0\n",
    "    return len(pred_keywords & ref_keywords) / len(ref_keywords)\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    predictions = eval_preds.predictions\n",
    "    label_ids = eval_preds.label_ids\n",
    "\n",
    "    # 如果 predictions 是 tuple（例如包含 logits），取第一个作为 token ids\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # 如果是 logits（形如 [batch, seq_len, vocab_size]），则 argmax 得到 token ids\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = predictions.argmax(-1)\n",
    "\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "\n",
    "    for pred_ids, label in zip(predictions, label_ids):\n",
    "        label = [id for id in label if id != -100]\n",
    "        decoded_pred = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "        decoded_label = tokenizer.decode(label, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds.append(decoded_pred.strip())\n",
    "        decoded_labels.append(decoded_label.strip())\n",
    "\n",
    "    # 打印第一个样本的预测和标签\n",
    "    # print(\"\\n==== 示例输出 ====\")\n",
    "    # print(\"预测：\", decoded_preds[0])\n",
    "    # print(\"标签：\", decoded_labels[0])\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1_scores = []\n",
    "    bleu4_scores = []\n",
    "    rouge_l_scores = []\n",
    "    keyword_overlaps = []\n",
    "    choice_correct = []\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        reference = label.split()\n",
    "        candidate = pred.split()\n",
    "\n",
    "        # BLEU-1 和 BLEU-4\n",
    "        bleu1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu4 = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "        # ROUGE-L\n",
    "        try:\n",
    "            rouge_l = rouge.get_scores(pred, label)[0]['rouge-l']['f']\n",
    "        except ValueError:\n",
    "            rouge_l = 0.0\n",
    "        rouge_l_scores.append(rouge_l)\n",
    "\n",
    "        # Keyword overlap\n",
    "        keyword_acc = keyword_overlap(pred, label)\n",
    "        keyword_overlaps.append(keyword_acc)\n",
    "\n",
    "        # Choice accuracy\n",
    "        pred_choice, _ = parse_output(pred)\n",
    "        label_choice, _ = parse_output(label)\n",
    "        is_correct = (pred_choice == label_choice) and (pred_choice != -1)\n",
    "        choice_correct.append(int(is_correct))\n",
    "\n",
    "    return {\n",
    "        \"BLEU-1\": np.mean(bleu1_scores),\n",
    "        \"BLEU-4\": np.mean(bleu4_scores),\n",
    "        \"ROUGE-L\": np.mean(rouge_l_scores),\n",
    "        \"KeywordOverlap\": np.mean(keyword_overlaps),\n",
    "        \"ChoiceAccuracy\": np.mean(choice_correct),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3586bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "small_train_dataset=dataset_train[:100]\n",
    "small_val_dataset=dataset_val[:5]\n",
    "train_dataset = QwenVLPrefixDataset(small_train_dataset, processor, debug=False)\n",
    "val_dataset = QwenVLPrefixDataset(small_val_dataset, processor, debug=False)    \n",
    "# dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804580d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_354847/448615443.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[codecarbon ERROR @ 14:48:35] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5vl-Lora1-6_0610\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20, \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1, \n",
    "    dataloader_num_workers=0,\n",
    "    eval_accumulation_steps=1,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,  # 如果使用的是支持 bfloat16 的 GPU，可改为 bf16=True\n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=qwen_vl_collate_fn,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer=processor.tokenizer),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21c2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:48:35] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:49, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu-1</th>\n",
       "      <th>Bleu-4</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Keywordoverlap</th>\n",
       "      <th>Choiceaccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.882800</td>\n",
       "      <td>2.933081</td>\n",
       "      <td>0.633390</td>\n",
       "      <td>0.551097</td>\n",
       "      <td>0.761899</td>\n",
       "      <td>0.921052</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.640300</td>\n",
       "      <td>2.871847</td>\n",
       "      <td>0.590472</td>\n",
       "      <td>0.518662</td>\n",
       "      <td>0.741928</td>\n",
       "      <td>0.933462</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.638300</td>\n",
       "      <td>2.878922</td>\n",
       "      <td>0.585641</td>\n",
       "      <td>0.522042</td>\n",
       "      <td>0.733815</td>\n",
       "      <td>0.937381</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:59:34] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=3.1733709653218587, metrics={'train_runtime': 658.9777, 'train_samples_per_second': 0.759, 'train_steps_per_second': 0.091, 'total_flos': 8590162057900032.0, 'train_loss': 3.1733709653218587, 'epoch': 4.64})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cc9f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./qwen2.5vl-Lora1-6/final_0609_10epoch\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
